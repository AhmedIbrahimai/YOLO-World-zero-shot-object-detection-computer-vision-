{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023afb97-fd1d-4d4e-ae32-c1e37716dfe6",
   "metadata": {},
   "source": [
    "## YOLO-WORLD\n",
    "\n",
    "YOLO-WORLD paper:  https://arxiv.org/pdf/2401.17270v2.pdf\n",
    "\n",
    "Source code: https://github.com/AILab-CVC/YOLO-World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f885db-3fac-49c6-af77-774d2b6ede84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed1de09-a502-4194-bf5b-1d1d4b78f675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.1.25'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bea651-24f9-4068-929b-7df5d8d8c831",
   "metadata": {},
   "source": [
    "### loading a pre-trained yolo-world model and running a prediction on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "423170a2-db8f-4803-b2ef-7dae42ffd21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\newon\\Downloads\\img.jpg: 448x640 3 persons, 2 cars, 1 stop sign, 1 backpack, 283.2ms\n",
      "Speed: 1.9ms preprocess, 283.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLOWorld\n",
    "\n",
    "# Initialize a YOLO-World model\n",
    "model = YOLOWorld('yolov8s-world.pt')  # or select yolov8m/l-world.pt for different sizes\n",
    "\n",
    "# Execute inference with the YOLOv8s-world model on the specified image\n",
    "results = model.predict('img.jpg', device='cpu', save=True)\n",
    "\n",
    "# Show results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c754039-f7d1-4b2e-9ad4-3d8bb8a17fad",
   "metadata": {},
   "source": [
    "### Set prompts for specific tasks\n",
    "\n",
    "The YOLO-World framework allows for the dynamic specification of classes through custom prompts, empowering users to tailor the model to their specific needs without retraining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c47f5879-3bd0-4c5a-93d1-43b940f01524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\newon\\Downloads\\img.jpg: 448x640 3 persons, 2 cars, 4 shoess, 335.2ms\n",
      "Speed: 5.0ms preprocess, 335.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize a YOLO-World model\n",
    "model = YOLO('yolov8s-world.pt')  # or choose yolov8m/l-world.pt\n",
    "\n",
    "# Define custom classes\n",
    "model.set_classes([\"person\", \"car\",\"shoes\"])\n",
    "\n",
    "# Execute prediction for specified categories on an image\n",
    "results = model.predict('img.jpg', device='cpu', save=True)\n",
    "\n",
    "# Show results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4519643-9e09-4aa7-87d0-501e1459709b",
   "metadata": {},
   "source": [
    "#### You can also save a model after setting custom classes. By doing this you create a version of the YOLO-World model that is specialized for your specific use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee229783-2fad-4a2c-8908-ec6ecabfe318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize a YOLO-World model\n",
    "model = YOLO('yolov8s-world.pt')  # or select yolov8m/l-world.pt\n",
    "\n",
    "# Define custom classes\n",
    "model.set_classes([\"person\"])\n",
    "\n",
    "# Save the model with the defined offline vocabulary\n",
    "model.save(\"custom_yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9dc3f9-8aee-45a7-a568-bc75ea186a57",
   "metadata": {},
   "source": [
    "##### After saving, the custom_yolov8s.pt model behaves like any other pre-trained YOLOv8 model but with a key difference: it is now optimized to detect only the classes you have defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35740f1-0759-420d-98bc-dd2dbd6f0aaa",
   "metadata": {},
   "source": [
    "## Load custom model and perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd0e7c98-0bbd-4b3d-a546-c299689f7bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\newon\\Downloads\\img.jpg: 448x640 3 persons, 287.6ms\n",
      "Speed: 3.0ms preprocess, 287.6ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your custom model\n",
    "model = YOLO('custom_yolov8s.pt')\n",
    "\n",
    "# Run inference to detect your custom classes\n",
    "results = model.predict('img.jpg', device='cpu')\n",
    "\n",
    "# Show results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "948c51cf-33ee-46fd-9837-1c946e98ce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success ✅ (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "\n",
      "WARNING ⚠️ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 1 person, 1 couch, 383.4ms\n",
      "0: 480x640 1 person, 1 couch, 1 laptop, 309.2ms\n",
      "0: 480x640 1 person, 1 couch, 319.2ms\n",
      "0: 480x640 1 person, 1 couch, 299.8ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 1 tv, 289.6ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 271.8ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 274.4ms\n",
      "0: 480x640 1 person, 1 couch, 272.4ms\n",
      "0: 480x640 1 person, 1 couch, 281.1ms\n",
      "0: 480x640 1 person, 1 couch, 294.4ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 267.8ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 272.5ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 272.8ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 264.6ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 281.6ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 278.1ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 257.8ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 269.0ms\n",
      "0: 480x640 1 person, 1 couch, 255.9ms\n",
      "0: 480x640 1 person, 1 couch, 274.5ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 273.0ms\n",
      "0: 480x640 1 person, 1 chair, 1 couch, 302.8ms\n",
      "0: 480x640 1 person, 1 couch, 297.2ms\n",
      "0: 480x640 2 persons, 288.8ms\n",
      "0: 480x640 1 person, 295.1ms\n",
      "0: 480x640 1 person, 274.8ms\n",
      "0: 480x640 2 persons, 336.7ms\n",
      "0: 480x640 2 persons, 320.2ms\n",
      "0: 480x640 1 person, 1 bed, 316.7ms\n",
      "0: 480x640 2 persons, 2 beds, 314.1ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 284.5ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 280.4ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 264.8ms\n",
      "0: 480x640 1 person, 1 hot dog, 1 donut, 1 chair, 1 bed, 288.8ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 272.4ms\n",
      "0: 480x640 2 persons, 2 beds, 254.5ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 295.9ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 267.1ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 1 cell phone, 289.1ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 255.0ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 274.1ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 1 cell phone, 272.4ms\n",
      "0: 480x640 2 persons, 1 donut, 1 chair, 1 bed, 276.3ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 284.0ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 1 cell phone, 281.6ms\n",
      "0: 480x640 2 persons, 1 pizza, 1 donut, 1 chair, 1 bed, 262.6ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 256.3ms\n",
      "0: 480x640 2 persons, 1 hot dog, 1 pizza, 1 donut, 1 chair, 2 beds, 274.5ms\n",
      "0: 480x640 2 persons, 1 hot dog, 1 pizza, 1 donut, 1 chair, 2 beds, 263.6ms\n",
      "0: 480x640 2 persons, 1 hot dog, 1 chair, 2 beds, 272.2ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 277.9ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 270.9ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 256.6ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 272.6ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 275.9ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 271.6ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 1 cell phone, 272.2ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 1 cell phone, 282.4ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 1 cell phone, 272.3ms\n",
      "0: 480x640 2 persons, 1 donut, 1 chair, 1 bed, 297.7ms\n",
      "0: 480x640 2 persons, 1 hot dog, 1 donut, 1 chair, 2 beds, 277.3ms\n",
      "0: 480x640 2 persons, 1 pizza, 1 donut, 1 chair, 244.6ms\n",
      "0: 480x640 2 persons, 2 beds, 261.6ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 250.1ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 257.5ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 302.0ms\n",
      "0: 480x640 2 persons, 1 hot dog, 2 beds, 282.5ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 272.9ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 271.6ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 272.2ms\n",
      "0: 480x640 2 persons, 2 beds, 247.6ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 280.5ms\n",
      "0: 480x640 2 persons, 2 beds, 293.8ms\n",
      "0: 480x640 2 persons, 1 donut, 1 chair, 2 beds, 287.9ms\n",
      "0: 480x640 2 persons, 2 beds, 1 cell phone, 285.8ms\n",
      "0: 480x640 2 persons, 1 pizza, 2 beds, 318.8ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 2 cell phones, 306.8ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 1 cell phone, 297.5ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 304.2ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 304.4ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 305.5ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 287.6ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 299.7ms\n",
      "0: 480x640 2 persons, 1 donut, 1 chair, 1 bed, 292.5ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 266.3ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 280.4ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 285.7ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 276.6ms\n",
      "0: 480x640 2 persons, 2 beds, 279.4ms\n",
      "0: 480x640 2 persons, 2 beds, 263.1ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 257.9ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 1 cell phone, 271.0ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 279.5ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 1 cell phone, 280.1ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 264.6ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 264.7ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 268.2ms\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 277.2ms\n",
      "0: 480x640 2 persons, 1 bed, 271.6ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 284.3ms\n",
      "0: 480x640 2 persons, 1 chair, 2 beds, 289.1ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your custom model\n",
    "model = YOLO('yolov8s-world.pt')\n",
    "\n",
    "\n",
    "# Run inference to detect your custom classes\n",
    "results = model.predict(\"\", device='cpu', save=True)\n",
    "\n",
    "# Show results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e5af1-585a-43e3-ba37-a4882153f0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
